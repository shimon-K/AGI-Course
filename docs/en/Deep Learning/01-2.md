---

title: Machine Learning
---


1. [Linear regression and correlation](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-2#Linear regression and correlation)
2. [Classification](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-2#CLASSIFICATION)
3. [Generative verse Disciminative approaches](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-2#Generative verse Disciminative approaches)
4. [Multiple tasks](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-2#Multiple tasks)
5. [Machine Learning tasks](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-2#Machine Learning tasks)







<a id="Linear regression and correlation"> </a>
## Linear regression and correlation


### Linear correlation and covariance matrix
<iframe width="760" height="365" src="https://www.youtube.com/embed/532siGy0b50" title="Linear correlation and covariance matrix" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- The _"Multivariate Gaussian distribution"_ part of the video is based on [this](https://www.youtube.com/watch?app=desktop&v=gR3XMv4WkR8&ab_channel=LawrenceLeemis), [this](https://www.youtube.com/watch?v=eho8xH3E6mE&ab_channel=AlexanderIhler), [this](https://datascienceplus.com/understanding-the-covariance-matrix/), and [this](https://cs229.stanford.edu/section/gaussians.pdf; https://rich-d-wilkinson.github.io/MATH3030/7.1-definition-and-properties-of-the-mvn.html). See more details [here](https://medium.com/@irenemarkelic/understanding-the-bivariate-normal-distribution-b9c50cc4cec0).
- Note that besides $r$ or pearson linear correlation between continuous variables, there are other correlation measures for other variable types: cremer for ordinal or categorical variables, and spirmann for ordinal. While pearson is for interval or ratio scale variables. There are also other correlation coefficients, e.g. [here](https://medium.com/towards-data-science/a-new-coefficient-of-correlation-64ae4f260310).
- Note also, that a single gaussian distribution is too simplistic assumption about general data, hence usually we use mixture of gaussians, i.e. where there are several blobs to represent actual data. <br/>
It is formulized as a weighted sum of multiple Gaussian distributions, each representing a cluster or component within the data: $p(\mathbf{x})=\sum\limits_{i=1}^{K}\omega_i\mathcal{N}(\mathbf{x}\|\mu_i,\Sigma_i)$ where $\sum\limits_{i=1}^{K}\omega_i=1$ make sure that the mixture weights form a valid probability distribution. The parameters $\mu_i,\Sigma_i,\omega_i$ are learned, via any parameter approximation method. For example, via Expectation-Maximization (EM) algorithm, to maximize the likelihood of the observed data.
- This section is the base for the _Gaussian processes_ in the [Bayesian Learning]() section.


### Algebra clarifications
<iframe width="760" height="365" src="https://www.youtube.com/embed/TZ1Rm4iDqG0" title="Algebra clarifications" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- More about linear transformations: [here](https://medium.com/recreational-maths/2d-transformation-matrices-f804b9a0daf8), [here](https://people.math.harvard.edu/~knill/teaching/math19b_2011/handouts/lecture08.pdf), and [here](https://interactivetextbooks.tudelft.nl/linear-algebra/Chapter3/GeometryofLinearTransformations.html).
- More about: [SVD](https://medium.com/intuition/the-mathematical-and-geographic-understanding-of-singular-value-decomposition-svd-8da2297797c6), [PCA](https://medium.com/intuition/mathematical-understanding-of-principal-component-analysis-6c761004c2f8), [eigen topic](https://joseph-mellor1999.medium.com/the-cayley-hamilton-theorem-dc6986747b23), and a [more simple algebra](https://medium.com/bitgrit-data-science-publication/linear-algebra-concepts-every-data-scientist-should-know-18b00bd453dd).


### Linear regression


- More about [regularizations](https://medium.com/intuition/understanding-l1-and-l2-regularization-with-analytical-and-probabilistic-views-8386285210fc), [kernels](https://medium.com/the-quantastic-journal/mathematical-understanding-of-gaussian-process-eaffc9c8a6d6), and [BLR](https://medium.com/intuition/gentle-introduction-of-bayesian-linear-regression-c83da6b0d1f7).




<a id="CLASSIFICATION"> </a>
## Classification


### Linear classification and SVM

<iframe width="760" height="365" src="https://www.youtube.com/embed/EcXmMzuVRMs" title="Linear classification and SVM (partial)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>



- Note, that Generalized Linear Models (GLMs) are used for cases in which we do not assume the error in $y=f(x)+\epsilon$ is Gaussian distributed. See more in [here](https://en.wikipedia.org/wiki/Generalized_linear_model).
- See more about Perceptron [here](https://en.wikipedia.org/wiki/Perceptron).
- Note that similar dual derivation we could do for hard-SVM, only without the $\alpha_n<C$ constraint, hence it is also contain the inner product and kernel trick. It is simply without the slack variables.
- Note that we assume that we also have feature=1 for bias term, hence the constraint in the dual form is derived from its derivative being equal to 0.
- Note that SVM is good for small data, since kernel trick swap calculating in feature dimension to data dimension (i.e. data size), since we calculate kernel for all data points.
- General RBF defined as: $k(x_i,x_j) = k(\paralle x_i-x_j\paralle)$ .
<!-- - Just like SVM allow non-linear features, Generalized Additive Models (GAMs) allow for non-linear features, e.g. splines used in GAMs. See more in XAI course in AICourses.docx. Also while PCA assumes linear correlations within the data, for non-linear we use kernel-PCA, like hereâ€¦ (ADD THIS IDEA IN BAYESIAN SLIDES IN CUBE ILLUSTRATION OF KERNEL-REGRESSION-GP)-->

- Good SVM explanation [here](https://medium.com/@kuriko-iwai/building-soft-margin-kernel-svms-ddf41684d418), and also [here](https://tomerkoren.notion.site/Recitation-4-Convex-Functions-and-the-SVM-optimization-problem-1cd6faf627a280e08188eef59c451b9f).



### More classification methods



<a id="Generative verse Disciminative approaches"> </a>
## Generative verse Disciminative approaches





<a id="Multiple tasks"> </a>
## Multiple tasks




<a id="Machine Learning tasks"> </a>
## Machine Learning tasks
