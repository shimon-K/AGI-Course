---

title: Learning types
---


Here we present different Learning types.
1. Abstract
2. Active Learning
3. [Reinforcement Learning (RL)](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-8#RL)
   * On-policy and Off-policy learning
   * Policy-based methods
5. Multi-task Learning (MTL)
6. Meta-Learning
7. Continual/Life-long Learning
8. Online/Offline Learning
9. Network Architecture Search (NAS)
10. [Bayesian Learning (BL)](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-8#BL)
   * Bayes topics
   * Bayes in Deep Learning
   * Uncertainties
   * Optimization under uncertainty
   * Gaussian Process and Kernels
11. Summary


<a id="INTRO"> </a>
## Abstract


<a id="ACTIVE"> </a>
## Active Learning



<a id="RL"> </a>
## Reinforcement Learning (RL)


### On-policy and Off-policy learning


### Policy-based methods

- Note that RL methods split into two types: value-based (which we learned about in STATE SPACE slides in [here](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#DPMC) and [here](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#TD)) and policy-based which we learn here. The difference between these two types can be seen [here](https://medium.com/free-code-camp/a-brief-introduction-to-reinforcement-learning-7799af5840db).
- About policy-gradient and AC in [here](https://medium.com/data-science-collective/business-driven-reinforcement-learning-foundations-of-value-policy-methods-part-i-bc345bea8e88).



<a id="MTL"> </a>
## Multi-task Learning (MTL)

<a id="META"> </a>
## Meta-Learning

<a id="CONTINUAL"> </a>
## Continual/Life-long Learning

<a id="ONLINE"> </a>
## Online/Offline Learning


<a id="NAS"> </a>
## Network Architecture Search (NAS)


<a id="BL"> </a>
## Bayesian Learning (BL)


### Bayes topics


### Bayes in Deep Learning


### Uncertainties


### Optimization under uncertainty


### Gaussian Process and Kernels

#### Gaussian Process
<iframe width="760" height="365" src="https://www.youtube.com/embed/ApYfcQXqygg" title="Gaussian Process (GP)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- So how to draw the plot of mean function + uncertainty? Simply by calculating the mean and covariance in the train+test points.
- Note that it is actually and not $f^* \vert x^*$ GP posterior (see in black box). 
- See different GP distributions in [here](tinyurl.com/guide2gp or https://infallible-thompson-49de36.netlify.app/).
- More on GP [here](https://readmedium.com/en/but-what-is-a-gaussian-process-an-intuition-for-dummies-dd95ee20484a), [here](https://medium.com/panoramic/gaussian-processes-for-little-data-2501518964e4), and a python implementation [here](https://towardsdatascience.com/implement-a-gaussian-process-from-scratch-2a074a470bce).
- Great (with detailed math) GP [explanation](https://medium.com/data-science/understanding-gaussian-process-the-socratic-way-ba02369d804), following sparse/var GP versions: [here](https://medium.com/data-science/sparse-and-variational-gaussian-process-what-to-do-when-data-is-large-2d3959f430e7) and [here](https://medium.com/data-science/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4). Sparse GP is to reduce K dimensions, and variational as a way to avoid original K huge calculations. 
- More on BLR [here](https://cedar.buffalo.edu/~srihari/CSE574/Chap3/3.4-BayesianRegression.pdf) and [here](https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.5-Regularization.pdf).
- See _“The automatic statistician”_ in [here](https://www.youtube.com/watch?v=y0FgHOQhG4w&list=PL93aLKqThq4hbACqDja5QFuFKDcNtkPXz) at time ~ 1:34:00.



#### Kernels summary
<iframe width="760" height="365" src="https://www.youtube.com/embed/Nty5AlUuW6M" title="Kernels summary" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- The _“Kernel summary”_ is based on [here](https://medium.com/@ryassminh/math-for-ml-kernels-explained-simply-with-examples-a24f5fd0699c), [here](https://medium.com/data-science/kernels-everything-you-need-to-know-f5d255d95785) and [here](https://youtu.be/y0FgHOQhG4w?list=PL93aLKqThq4hbACqDja5QFuFKDcNtkPXz) (from time ~ 1:20:00).

<a id="SUMMARY"> </a>
## Summary
