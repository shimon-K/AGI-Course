---

title: Learning types
---


Here we present different Learning types.
1. Abstract
2. Active Learning
3. [Reinforcement Learning (RL)](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-8#RL)
   * On-policy and Off-policy learning
   * Policy-based methods
5. Multi-task Learning (MTL)
6. Meta-Learning
7. Continual/Life-long Learning
8. Online/Offline Learning
9. Network Architecture Search (NAS)
10. [Bayesian Learning (BL)](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-8#BL)
   * Bayes topics
   * Bayes in Deep Learning
   * Uncertainties
   * Optimization under uncertainty
   * Gaussian Process and Kernels
11. Summary


<a id="INTRO"> </a>
## Abstract


<a id="ACTIVE"> </a>
## Active Learning



### Neural ODE (NODE)
<iframe width="760" height="365" src="https://www.youtube.com/embed/-aPuqtyPPEE" title="PINN 01 - NODE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- Special NNs such as Hamiltonian NN for Hamiltonian equations or Lagrangian NN for Euler-Lagrange equations, can be found in the paper: _“Hamiltonian neural networks for solving equations of motion - Marios Mattheakis et al”_ and in the [video](https://www.youtube.com/watch?v=AEOcss20nDA&list=PLMrJAkhIeNNQ0BaKuBKY43k4xMo6NSbBa&index=16&ab_channel=SteveBrunton).
- This video also based on [this](https://towardsdatascience.com/modeling-dynamical-systems-with-neural-ode-a-hands-on-guide-71c4cfdb84dc) article, [this](https://towardsdatascience.com/solving-differential-equations-with-neural-networks-4c6aa7b31c51) article, and [this](https://towardsdatascience.com/physics-informed-neural-networks-an-application-centric-guide-dc1013526b02) article.

### PINNs
<iframe width="760" height="365" src="https://www.youtube.com/embed/x4nbHegRwo0" title="PINN 02 - PINNs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- This video also based on [this](https://towardsdatascience.com/physics-informed-neural-networks-an-application-centric-guide-dc1013526b02) article.
- Note that $f$ or the dynamic function is referred to as vector field. Why? Since it shows the trajectory (through time for ODE, and also through space for PDE) of any “particle” starting from some initial position.
- Enforcing causality in PINNs described in the [paper](https://arxiv.org/pdf/2203.07404).
- PINNs could be extended to fractional PINNs handling integrals and fractional derivatives (incomplete ones). Also delta-PINNs that incorporates geometry prior of the problem. See more in the [video](https://www.youtube.com/watch?v=-zrY7P2dVC4&ab_channel=SteveBrunton).
- A full online course about PI-ML in the [playlist](https://www.youtube.com/playlist?list=PLMrJAkhIeNNQ0BaKuBKY43k4xMo6NSbBa).



### RNN vs ODE
<iframe width="760" height="365" src="https://www.youtube.com/embed/smRXsXt54D8" title="PINN 03 - RNN vs ODE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

- Note that NODEs and PINNs without free parameters, are autonomous systems, i.e. determined deterministically by ICs and BCs. However, parameters are just like inputs to the system, that intervene/interfere the system, as also happens in RNNs, which in its turn affect the output. This idea is also illustrated in the state plane and input and output vectors interacting with it in [State space](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/) section.

<a id="RL"> </a>
## Reinforcement Learning (RL)


### On-policy and Off-policy learning


### Policy-based methods

- Note that RL methods split into two types: value-based (which we learned about in STATE SPACE slides in [here](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#DPMC) and [here](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#TD)) and policy-based which we learn here. The difference between these two types can be seen [here](https://medium.com/free-code-camp/a-brief-introduction-to-reinforcement-learning-7799af5840db).
- About policy-gradient and AC in [here](https://medium.com/data-science-collective/business-driven-reinforcement-learning-foundations-of-value-policy-methods-part-i-bc345bea8e88).



<a id="MTL"> </a>
## Multi-task Learning (MTL)

<a id="META"> </a>
## Meta-Learning

<a id="CONTINUAL"> </a>
## Continual/Life-long Learning

<a id="ONLINE"> </a>
## Online/Offline Learning


<a id="NAS"> </a>
## Network Architecture Search (NAS)


<a id="BL"> </a>
## Bayesian Learning (BL)


### Bayes topics


### Bayes in Deep Learning


### Uncertainties


### Optimization under uncertainty


### Gaussian Process and Kernels

#### Gaussian Process
<iframe width="760" height="365" src="https://www.youtube.com/embed/ApYfcQXqygg" title="Gaussian Process (GP)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

#### Kernels summary
<iframe width="760" height="365" src="https://www.youtube.com/embed/Nty5AlUuW6M" title="Kernels summary" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<a id="SUMMARY"> </a>
## Summary
