---

title: Probability and Statistics
---


Here we present AI-relevant Probability and Statistics mathematical necessary background.
1. [Abstract](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#INTRO)
2. [Events and Random Variables](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Events)
3. [Distributions](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Distributions)
4. [Statistics and distributions](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Statistics and distributions)
5. [Conditional probability](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Conditional probability)
6. [Expectance and Parameter estimation](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Expectance and Parameter estimation)
7. [Sampling methods](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Sampling methods)
8. [Infromation theory](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Infromation theory)
9. [Summary](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Summary)





<a id="INTRO"> </a>
## Abstract


- There are many online books and courses in probability. For example [here](https://www.youtube.com/watch?v=2MuDZIAzBMY&list=PLoROMvodv4rOpr_A7B9SriE_iZmkanvUg&index=1&pp=iAQB).

<a id="Events"> </a>
## Events and Random Variables

### Events
<iframe width="760" height="365" src="https://www.youtube.com/embed/0ZVlvGkWl-Q" title="Events" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- _Simple_ event = one possible outcome of an experiment; _Complex_ event = a group of possible outcomes of an experiment.

### Events and Random Variables
<iframe width="760" height="365" src="https://www.youtube.com/embed/YN87BnUKi_8" title="Events and Random Variables" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


<a id="Distributions"> </a>
## Distributions
<iframe width="760" height="365" src="https://www.youtube.com/embed/aIOwNwAxT8A" title="Distributions" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- More fundamentally about randomness is [here](https://www.cantorsparadise.com/creating-randomness-eb512756c9c7).
- Pdf and Cdf are related via $f(x)=\frac{d}{dx}F(x)$, and they are used for example in [Sampling](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Sampling methods).

<a id="Statistics and distributions"> </a>
## Statistics and distributions
<iframe width="760" height="365" src="https://www.youtube.com/embed/ni_CFo0eaRI" title="Statistics and distributions" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- Range could be defined either as [minimum value, maximum value] or as a single value describing it, e.g. the mean of a range.
- [Here](https://medium.com/@Brain_Boost/gaussian-distribution-vs-poisson-distribution-195f780a2b4) for example, is about Poisson (which fits a specifc type of variable).
- The example with dice was taken from [here](https://web.stanford.edu/class/archive/cs/cs109/cs109.1226/).
<!-- - If instead we take sum of Xj’s and divide by sqrt(n) instead of n, and by STD, and reduce the mean for each Xj, we get the standard normal distribution N(0,1).-->
- Unlike the dice example, going from sum of 1 variable, to a sum of 2 variables, and so on - we can see alternitavely calculating $X+Y$ by convolution of the $(X,Y)$ distribution. Performing it over and over multiple times derives the gaussian curve as CLT states. See this [here](https://www.youtube.com/watch?v=IaSGqQa5O-M&ab_channel=3Blue1Brown).
- More about Weak vs. Strong Law of large numbers can be found [here](https://readmedium.com/en/the-laws-of-large-numbers-af9f130ce5d0).
- Some sources for the "Bernoulli $\rightarrow$ Binomial $\rightarrow$ Normal distribution" derivation: [here](https://scipp.ucsc.edu/~haber/ph116C/NormalApprox.pdf), [here](https://www.youtube.com/watch?v=45K4kEXso2g), [here](https://www.m-hikari.com/imf/imf-2017/9-12-2017/p/baguiIMF9-12-2017.pdf), and [here](https://people.bath.ac.uk/pam28/Paul_Milewski,_Professor_of_Mathematics,_University_of_Bath/Past_Teaching_files/stirling.pdf).


<a id="Conditional probability"> </a>
## Conditional probability
<iframe width="760" height="365" src="https://www.youtube.com/embed/EKuSTQE3jzg" title="Conditional probability" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- If $A\perp B$ then also their completing counterparts and their combinations are also independent: $\overline{A}\perp B$ ; $A\perp \overline{B}$ ; $\overline{A}\perp \overline{B}$. 
- If $A$ and $B$ are mutually exlusive events (and $P(A),P(B)>0$),\
i.e. $P(A\cap B) = 0 \ne P(A)P(B)$, which means $A,B$ are not independent (i.e. dependent). And vice versa: $P(A\cap B) = P(A)P(B) \ne 0$.
- Note that all of what we talked about in this video is appropriate both for events or for random variables, since they are equivalent. And it is appropriate both for discrete probabilities ($p$) and continuous ones ($f$).


<a id="Expectance and Parameter estimation"> </a>
## Expectance and Parameter estimation

### Expectance
<!--In the Expectance slides:-->
<iframe width="760" height="365" src="https://www.youtube.com/embed/Kt9Aebd_NTY" title="Expectance" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<!--- Another explanation of expectance: if we experiment infinite times and each time measure $x$ or $h(x)$, the expectance would be its average value.
- Note that moments are not only similar to taylor approximation, but they actually can be extracted from taylor expansion of a Moment-generating function, as the derivatives around point 0 of different orders. This-->
- A _“Moment generating function”_ is defined as $M_X(t) = E[e^{tX}]$, whose derivatives are the moments. This is similar to the relation between cdf and pdf.
- Another argument in Expectance is: $E[g(Y)\|X]=E[X]$, also referred to as _“Law of total expectation”_. Also: $E[X]=\sum_{y}^{}\underbrace{E[X\|Y=y]}_\text{g(Y)}P(Y=y)=E[g(Y)]=E[E[X\|Y]]$, and $E[h(X)Y\|X]=h(X)E[Y\|X]$.
- We will see later more derivations:
  - $Var(X) = E[(X-E[X])^2] = E[X^2]-(E[X])^2$.
  - For $X,Y$ independent variables: $P(X,Y)=P(X)P(Y)$ or\
$f(X,Y)=f(X)f(Y)$, and $E[X\cdot Y]=E[X]E[Y]$. See this independence also in COVARIANCE definition in the [linear regression slide](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Linear regression and correlation). 
  - Also for new variable $Z=X+Y$: $E[Z]=E[X]+E[Y]$,\
$Var(Z)=Var(X)+Var(Y)+2\cdot Cov(X,Y)$.\
Also $Z$ ~ Convlolution of $p(X)$ and $p(Y)$ or $f(X)$ and $f(Y)$. See proof in [here](https://www.youtube.com/watch?v=IaSGqQa5O-M&ab_channel=3Blue1Brown).
- Note that expectance and (co)variance rules and charachteristics are similar also for conditional probability, since instead of given all possibilities ($\Omega$), the all possibilities shrink to some other known event, like a constraint over all possibilities.


### Empirical versus Theoretical models
<iframe width="760" height="365" src="https://www.youtube.com/embed/Mqd2CAj5jBM" title="Empirical versus Theoretical models" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- Note that the theoretical parameters could be also calculated for the continuous case. 
- See [here](https://en.wikipedia.org/wiki/Variance) why the empirical STD shown here is biased.
- See how the relative occurrence estimation is a solution of MLE for normal distribution assumption/prior of i.i.d data samples, [here](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Examples).

### Parameter estimation
<iframe width="760" height="365" src="https://www.youtube.com/embed/ZRYBnRV3IVs" title="Parameter Estimation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<!--In the Parameter estimation slides:-->
- More about the comparison of the three classification mehods is in [here](https://slideplayer.com/slide/10998954/) and [here](https://ml.cs.tsinghua.edu.cn/~jun/courses/statml-fall2015/5-NB-Logistic%20Regression.pdf).
- Naive Bayes and Logistic regression can be compared via graphical representation: Bayesian or Causal Networks, see in [State Space](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/) section.

### Bayes rule demonstration
<iframe width="760" height="365" src="https://www.youtube.com/embed/5nf3KdsNjQY" title="Bayes rule demonstration" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- The second part of the video is based on [this](https://www.researchgate.net/publication/361507494_Individual_beliefs_about_temporal_continuity_explain_variation_of_perceptual_biases) paper.
- For more about it, see [here](https://econ.pages.code.wm.edu/414/notes/docs/intro_bayesian_statistics_2.html), and [here](https://towardsdatascience.com/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6).


<a id="Sampling methods"> </a>
## Sampling methods


<a id="Infromation theory"> </a>
## Infromation theory



<a id="Summary"> </a>
## Summary
