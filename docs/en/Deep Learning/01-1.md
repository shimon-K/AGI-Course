---

title: Probability and Statistics
---


Here we present AI-relevant Probability and Statistics mathematical necessary background.
1. [Abstract](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#INTRO)
2. [Events and Random Variables](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Events)
3. [Distributions](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Distributions)
4. [Statistics and distributions](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Statistics and distributions)
5. [Conditional probability](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Conditional probability)
6. [Expectance and Parameter estimation](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Expectance and Parameter estimation)
7. [Linear regression and correlation](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Linear regression and correlation)
8. [Machine Learning tasks](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Machine Learning tasks)
9. [Sampling methods](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Sampling methods)
10. [Infromation theory](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Infromation theory)
11. [Summary](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Summary)





<a id="INTRO"> </a>
## Abstract


- There are many online books and courses in probability. For example [here](https://www.youtube.com/watch?v=2MuDZIAzBMY&list=PLoROMvodv4rOpr_A7B9SriE_iZmkanvUg&index=1&pp=iAQB).

<a id="Events"> </a>
## Events and Random Variables

### Events
<iframe width="760" height="365" src="https://www.youtube.com/embed/jNktOAAU6Ys" title="Events" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- _Simple_ event = one possible outcome of an experiment; _Complex_ event = a group of possible outcomes of an experiment.

### Events and Random Variables
<iframe width="760" height="365" src="https://www.youtube.com/embed/YN87BnUKi_8" title="Events and Random Variables" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


<a id="Distributions"> </a>
## Distributions
<iframe width="760" height="365" src="https://www.youtube.com/embed/aIOwNwAxT8A" title="Distributions" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- More fundamentally about randomness is [here](https://www.cantorsparadise.com/creating-randomness-eb512756c9c7).
- Pdf and Cdf are used for example in [Sampling](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Sampling methods).

<a id="Statistics and distributions"> </a>
## Statistics and distributions
- Range could be defined either as [minimum value, maximum value] or as a single value describing it, e.g. the mean of a range.
- [Here](https://medium.com/@Brain_Boost/gaussian-distribution-vs-poisson-distribution-195f780a2b4) for example, is about Poisson (which fits a specifc type of variable).
- The example with dice was taken from [here](https://web.stanford.edu/class/archive/cs/cs109/cs109.1226/).
<!-- - If instead we take sum of Xj’s and divide by sqrt(n) instead of n, and by STD, and reduce the mean for each Xj, we get the standard normal distribution N(0,1).-->
- Unlike the dice example, going from sum of 1 variable, to a sum of 2 variables, and so on - we can see alternitavely calculating $X+Y$ by convolution of the $(X,Y)$ distribution. Performing it over and over multiple times derives the gaussian curve as CLT states. See this [here](https://www.youtube.com/watch?v=IaSGqQa5O-M&ab_channel=3Blue1Brown).
- More about Weak vs. Strong Law of large numbers can be found [here](https://readmedium.com/en/the-laws-of-large-numbers-af9f130ce5d0).
- Some sources for the "Bernoulli $\rightarrow$ Binomial $\rightarrow$ Normal distribution" derivation: [here](https://scipp.ucsc.edu/~haber/ph116C/NormalApprox.pdf), [here](https://www.youtube.com/watch?v=45K4kEXso2g), [here](https://www.m-hikari.com/imf/imf-2017/9-12-2017/p/baguiIMF9-12-2017.pdf), and [here](https://people.bath.ac.uk/pam28/Paul_Milewski,_Professor_of_Mathematics,_University_of_Bath/Past_Teaching_files/stirling.pdf).


<a id="Conditional probability"> </a>
## Conditional probability
- If $A\perp B$ then also their completing counterparts and their combinations are also independent: $\overline{A}\perp B$ ; $A\perp \overline{B}$ ; $\overline{A}\perp \overline{B}$. 
- Note that all of what we talked about in this video is appropriate both for events or for random variables, since they are equivalent. And it is appropriate both for discrete probabilities ($p$) and continuous ones ($f$).


<a id="Expectance and Parameter estimation"> </a>
## Expectance and Parameter estimation
In the Expectance slides:
- Another argument in Expectance is: $E[g(Y)\|X]=E[X]$, also referred to as _“Law of total expectation”_.
- We will see later more derivations:
  - $Var(X)=E[X^2]-(E[X])^2$.
  - For $X,Y$ independent variables: $P(X,Y)=P(X)P(Y)$ or 
$f(X,Y)=f(X)f(Y)$, and $E[X\cdot Y]=E[X]E[Y]$. See this independence also in COVARIANCE definition in the [linear regression slide](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Linear regression and correlation). 
  - Also for new variable $Z=X+Y$: $E[Z]=E[X]+E[Y]$, 
$Var(Z)=Var(X)+Var(Y)+2\cdot Cov(X,Y)$. 
Also $Z$ ~ Convlolution of $p(X)$ and $p(Y)$ or $f(X)$ and $f(Y)$. See proof in [here](https://www.youtube.com/watch?v=IaSGqQa5O-M&ab_channel=3Blue1Brown).
- Note that expectance and (co)variance rules and charachteristics are similar also for conditional probability, since instead of given all possibilities ($\Omega$), the all possibilities shrink to some other known event, like a constraint over all possibilities.
- Note that there is also _“Moment generating function”_, whose derivative is the moments. This is similar to the relation between cdf and pdf.

In the Parameter estimation slides:
- bla bla...


<a id="Linear regression and correlation"> </a>
## Linear regression and correlation

<a id="Machine Learning tasks"> </a>
## Machine Learning tasks


<a id="Sampling methods"> </a>
## Sampling methods


<a id="Infromation theory"> </a>
## Infromation theory



<a id="Summary"> </a>
## Summary
