---

title: State Space
---



Here we present the state space topic in the different AI fields.
1. [Abstract](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#INTRO)
2. [State space introduction](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#SS_Intro)
3. [State space in different fields](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#SS_Fields)
4. [State space in Classic AI](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#SS_Classic)
5. [Different state transition models](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#trans)
6. [Different policy learning techniques - DP and MC](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#DPMC)
7. [Different policy learning techniques - TD and more](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#TD)
8. [Summary](https://shimon-k.github.io/AGI-Course/en/Classical%20AI/01-5/#Summary)





<a id="INTRO"> </a>
## Abstract

<iframe width="760" height="365" src="https://www.youtube.com/embed/VTJ35B48-Ig?list=PLvii8t7-YebhvKN09vGfN8-956YDqFIrV" title="Abstract" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>



<a id="SS_Intro"> </a>
## State space introduction

<iframe width="760" height="365" src="https://www.youtube.com/embed/xtxj7rZa4iM?list=PLvii8t7-YebhvKN09vGfN8-956YDqFIrV" title="State space introduction" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- See about purposefulness [here](https://medium.com/the-infinite-universe/purpose-may-be-an-emergent-property-of-life-6c741eb67f5a).

<a id="SS_Fields"> </a>
## State space in different fields
<iframe width="760" height="365" src="https://www.youtube.com/embed/DcTOW8G0240?list=PLvii8t7-YebhvKN09vGfN8-956YDqFIrV" title="State space in different fields" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- In the k-Nearest-Neighbor method, the clustering depends on metrics! How do you define $d(x)$ distance between $x$’s? This is prior knowledge.
- In decision trees, the "after-learning" is made out of rectangles, since it is based on if-else nested trees.
- More about searching in hypotheses space look in this [section](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-4), as an intro to inductive bias topic.


<a id="SS_Classic"> </a>
## State space in Classic AI
<iframe width="760" height="365" src="https://www.youtube.com/embed/0bmrM8ZndLI?list=PLvii8t7-YebhvKN09vGfN8-956YDqFIrV" title="State space in Classic AI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- Some acronyms: dynamic programming (DP), temporal difference (TD), Monte Carlo (MC). Control methods: model predictive control (MPC), interpolating control (IC).
- The reason we plan for $N$ steps while implementing only the first one, as in MPC, is that we want to reach our goal state, thus making sure the next step we perform is getting us there. Hence, this is a sub-optimal solution. Optimally, we would consider all steps till the goal state, i.e. $N=\infty$.
- Extensions of A\*: A\*$\epsilon$ is a version of A\*, where it guarantees approximate solution, $(1-\epsilon) \times$ optimal solution, since it picks not only the minimal node to branch from, but from minimal $\times (1-\epsilon)$. IDA\* is a variation of A\* with a memory limit for all the nodes in the front of the search. Note also that $g, h$ should be on the same scale.
- <ins>Changing will</ins>: in numeric state representation we could dedicate special features for will, which can also act as direction features, to indicate where is preferred to move according to the current will. In function representation, it is via changing the function. In formal logic representation, it is by changing the will-specific features to indicate which direction/actions are preferred. All these representations assume that the will/request is embedded in the state, similar to the prompt to LM (e.g. ChatGPT), where we both include the given state (description of the problem) and what we want it to do (description of the goal).
- Note that A\* is like the superposition of origin and destination point fields, just like the will field. That is, at each point we sum the value according to the origin state and the value according to the destination field. See more [here](https://www.youtube.com/watch?v=LCDofAE5gYw&ab_channel=DiscoverAI).
- Another [method](https://mark-burgess-oslo-mb.medium.com/searching-in-graphs-artificial-reasoning-and-quantum-loop-corrections-with-semantics-spacetime-ea8df54ba1c5) for combining forward and backward chaining is straightforward search from beginning and end specific points and finding where they meet, unlike A\* which starts only from beginning point. Called "waves on a graph". It is also suggested non-deterministically, via quantum theory, see also in Consiosuncess slide, where two waves propogate - one from the past and one from the future to meet in the present.




<a id="trans"> </a>
## Different state transition models
<iframe width="760" height="365" src="https://www.youtube.com/embed/GF-miYs6Qms?list=PLvii8t7-YebhvKN09vGfN8-956YDqFIrV" title="Different state transition models" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- <h1>See the beauty of connecting seperate topics: system dynamics from control theory as deterministic and Markov chains as stochastic and causality as Bayesian networks and Do operations.</h1>
- RL is mainly based on [here](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html).
- In Markov models: We can infer forward using variable elimination, from previous state transitions. 
But we can also infer from future state-transitions, such as in HMM: infer $P(x_t|Y)$ or $P(x_t,x_{t+1}|Y)$ where $Y$ are past and future observations and $x$ states are hidden.
- Another definition: _Passive RL_ is when I have recorded the agent’s actions, transitions, and rewards, while _Active RL_ can explore more actions, that are not in our policy, versus exploiting what is in the current best policy as in _Passive RL_. 
- Note, that in playing chess, though my action is optimal and hence deterministic, the resulting next state of my next turn is dependent on my component, and it is stochastic.
- We can regard the state-to-state transition as internal dynamics, and state-to-output as external “dynamics”.
- In control, there is a cost/objective function, which determines the goal of the control. Its destination. In RL it’s simply the reward (current+discounted future rewards). Linear–quadratic regulator (LQR) for example has a quadratic cost and linear dynamics and constraints, results with off-line control policy, i.e. action $a$ is derived directly from the state. But non-linear cost yields an online LQR (iLQR=iterative LQR). Similarly, MPC, IC, and SIC are online control solutions.
- The controlled Markov chains are called stationary if the $P$ doesn’t depend on time.
- In automats theory, state is state, and action=none/input.
- See also [here](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12348/2641841/Variants-of-Bellman-equation-on-reinforcement-learning-problems/10.1117/12.2641841.full?SSO=1).
- More on Mamba: [here](https://medium.com/towards-data-science/towards-mamba-state-space-models-for-images-videos-and-time-series-1e0bfdb5933a) and [here](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state).


<a id="DPMC"> </a>
## Different policy learning techniques - DP and MC

<iframe width="760" height="365" src="https://www.youtube.com/embed/dsHyKeLOKu4?list=PLvii8t7-YebhvKN09vGfN8-956YDqFIrV" title="Different policy learning techniques - DP and MC" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- Based on RL course series [here](https://www.coursera.org/specializations/reinforcement-learning).
- More about Monte Carlo [here](https://towardsdatascience.com/mastering-monte-carlo-how-to-simulate-your-way-to-better-machine-learning-models-6b57ec4e5514) and [here](https://www.youtube.com/watch?v=qsHkZI_Bf-s&ab_channel=CompuFlair).
- Also see [this](https://www.deeplearningbook.org/contents/monte_carlo.html) chapter in [this](https://www.deeplearningbook.org/) book.
- MCTS: Note that MCTS does not have to retain approximate values/policies from one action selection to the next, since it updates the value estimates of nodes along the path to reflect the win percentage of simulations passing through that node. Or: Unlike standard MC in RL that is used to learn value functions and policies in unknown environments, MCTS is used for planning/decision-making in known environments (e.g., games) and it does not require complete episodes; instead, it simulates rollouts (via some special policy, e.g. random one) from a current state to explore the best moves. What is updated is the win/loss counts/statistics up the tree.



<a id="TD"> </a>
## Different policy learning techniques - TD and more

<iframe width="760" height="365" src="https://www.youtube.com/embed/S9mdSSMGySE?list=PLvii8t7-YebhvKN09vGfN8-956YDqFIrV" title="Different policy learning techniques - TD and more" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- From [here](https://medium.com/aimonks/exploring-temporal-difference-learning-a-paradigm-shift-in-reinforcement-learning-782e91d1b29c): “However, unlike Monte Carlo approaches that only update value estimates at the end of an episode, TD methods continually update estimates based on other estimates, a process known as _bootstrapping_.”
- See about TD [here](https://arshren.medium.com/reinforcement-learning-temporal-difference-learning-e8c1e1fbc91e) and about TD in [neuroscience](https://readmedium.com/en/deep-reinforcement-learning-toward-integrated-and-unified-ai-823f665ed909).
- Also see the last file about RL in some course's [folder](https://drive.google.com/drive/folders/1CyRAI2ED1GekUsT6qAgOcYhv7WUbFBJH).
- Another comparison of the methods see [here](https://medium.com/towards-data-science/deep-reinforcement-learning-toward-integrated-and-unified-ai-823f665ed909).




<a id="Summary"> </a>
## Summary

<iframe width="760" height="365" src="https://www.youtube.com/embed/sdJVt79hfk8?list=PLvii8t7-YebhvKN09vGfN8-956YDqFIrV" title="Summary" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
