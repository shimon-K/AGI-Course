---

title: State Space
---



Here we present AI-relevant Probability and Statistics mathematical necessary background.
1. [Abstract](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#INTRO)
2. [State space introduction](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#SS_Intro)
3. [State space in different fields](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#SS_Fields)
4. [State space in Classic AI](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#SS_Classic)
5. [Different state transition models](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#trans)
6. [Different policy learning techniques - DP and MC](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#DPMC)
7. [Different policy learning techniques - TD and more](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#TD)
8. [Summary](https://shimon-k.github.io/AGI-Course/en/Deep%20Learning/01-1#Summary)





<a id="INTRO"> </a>
## Abstract





<a id="SS_Intro"> </a>
## State space introduction

<iframe width="760" height="365" src="https://www.youtube.com/embed/0ZVlvGkWl-Q" title="Events" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- See about purposefullness in [here](https://medium.com/the-infinite-universe/purpose-may-be-an-emergent-property-of-life-6c741eb67f5a).

<a id="SS_Fields"> </a>
## State space in different fields
<iframe width="760" height="365" src="https://www.youtube.com/embed/aIOwNwAxT8A" title="Distributions" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- In decision trees, the "after-learning" is made out of rectangles, since it is based on if-else nested trees.
- More about searching in Hypotheses space look in Deep Learning, in the "different neural networks" section, as intro to inductive bias topic.


<a id="SS_Classic"> </a>
## State space in Classic AI
<iframe width="760" height="365" src="https://www.youtube.com/embed/ni_CFo0eaRI" title="Statistics and distributions" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- Some acronyms: dynamic programming (DP), temporal difference (TD), Monte Carlo (MC). Control mehods: model predictive control (MPC), interpolating control (IC).
- [Here](https://medium.com/@Brain_Boost/gaussian-distribution-vs-poisson-distribution-195f780a2b4) for example, is about Poisson (which fits a specifc type of variable).
- The example with dice was taken from [here](https://web.stanford.edu/class/archive/cs/cs109/cs109.1226/).
<!-- - If instead we take sum of Xj’s and divide by sqrt(n) instead of n, and by STD, and reduce the mean for each Xj, we get the standard normal distribution N(0,1).-->
- Unlike the dice example, going from sum of 1 variable, to a sum of 2 variables, and so on - we can see alternitavely calculating $X+Y$ by convolution of the $(X,Y)$ distribution. Performing it over and over multiple times derives the gaussian curve as CLT states. See this [here](https://www.youtube.com/watch?v=IaSGqQa5O-M&ab_channel=3Blue1Brown).
- More about Weak vs. Strong Law of large numbers can be found [here](https://readmedium.com/en/the-laws-of-large-numbers-af9f130ce5d0).
- Some sources for the "Bernoulli $\rightarrow$ Binomial $\rightarrow$ Normal distribution" derivation: [here](https://scipp.ucsc.edu/~haber/ph116C/NormalApprox.pdf), [here](https://www.youtube.com/watch?v=45K4kEXso2g), [here](https://www.m-hikari.com/imf/imf-2017/9-12-2017/p/baguiIMF9-12-2017.pdf), and [here](https://people.bath.ac.uk/pam28/Paul_Milewski,_Professor_of_Mathematics,_University_of_Bath/Past_Teaching_files/stirling.pdf). Also, there is _t-distribution_, which generalizes the standard normal distribution where the sample size is too small.


<a id="trans"> </a>
## Different state transition models
<iframe width="760" height="365" src="https://www.youtube.com/embed/EKuSTQE3jzg" title="Conditional probability" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
- RL is mainly based on [here](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html).
- Another definition: _Passive RL_ is when I have recorded agent’s actions, transitions and rewards, while _Active RL_ can explore more actions, that are not in our policy, versus exploiting what is in the current best policy as in _Passive RL_. 
- Note, that in playing chess, though my action is optimal hence deterministic, the resulted next state of my next turn is dependent on my component, and it is stochastic.
- We can regard the state to state transition as internal dynamics, and state to output as external “dynamics”.
- In control, there is cost/objective function, which determines the goal of the control. It’s destination. In RL it’s simply the reward (current+discounted future rewards). Linear–quadratic regulator (LQR) for example have quadratic cost and linear dynamics and constraints, results with off-line control policy, i.e. action $a$ is derived directly from state. But non-linear cost yields an on-line LQR (iLQR=iterative LQR). Similarly MPC, IC, SIC are on-line control solutions.
- The controlled Markov chains called stationary if the $P$ isn’t depend on time.
- In automats theory, state is state, and action=none/input.
- See also [here](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12348/2641841/Variants-of-Bellman-equation-on-reinforcement-learning-problems/10.1117/12.2641841.full?SSO=1).
- More on Mamba: [here](https://medium.com/towards-data-science/towards-mamba-state-space-models-for-images-videos-and-time-series-1e0bfdb5933a) and [here](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state).


<a id="DPMC"> </a>
## Different policy learning techniques - DP and MC
- Based on RL course series [here](https://www.coursera.org/specializations/reinforcement-learning).

<a id="TD"> </a>
## Different policy learning techniques - TD and more
- From [here](https://medium.com/aimonks/exploring-temporal-difference-learning-a-paradigm-shift-in-reinforcement-learning-782e91d1b29c): “However, unlike Monte Carlo approaches that only update value estimates at the end of an episode, TD methods continually update estimates based on other estimates, a process known as _bootstrapping_.”
- See about TD [here](https://arshren.medium.com/reinforcement-learning-temporal-difference-learning-e8c1e1fbc91e) and about TD in [neuroscience](https://readmedium.com/en/deep-reinforcement-learning-toward-integrated-and-unified-ai-823f665ed909).
- Also see last file about RL in some course's [folder](https://drive.google.com/drive/folders/1CyRAI2ED1GekUsT6qAgOcYhv7WUbFBJH).
- Also see [this](https://www.deeplearningbook.org/contents/monte_carlo.html) chapter in [this](https://www.deeplearningbook.org/) book.
- More about Monte Carlo [here](https://towardsdatascience.com/mastering-monte-carlo-how-to-simulate-your-way-to-better-machine-learning-models-6b57ec4e5514) and [here](https://www.youtube.com/watch?v=qsHkZI_Bf-s&ab_channel=CompuFlair).
- Another comparison of the methods see [here](https://medium.com/towards-data-science/deep-reinforcement-learning-toward-integrated-and-unified-ai-823f665ed909).




<a id="Summary"> </a>
## Summary
